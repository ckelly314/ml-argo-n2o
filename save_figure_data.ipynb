{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41338997-bbfe-467b-84d6-e17196d07c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.path as mpath\n",
    "import scipy.io\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# functions from gasex module\n",
    "from gasex.phys import vpress_sw\n",
    "from gasex.fugacity import fugacity_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fd9410-a9da-405d-8727-78263acb1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def generateinputs(tests, windproduct):\n",
    "    parameterizations = [\"W14\", \"L13\"]\n",
    "    #windproducts = [\"ncep\"]    \n",
    "    inputs = [[p,w, t] for p in parameterizations for w in windproduct for t in tests]\n",
    "    print(f\"inputs: {inputs}\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5651b5-e8eb-4f4f-a124-631a51b29796",
   "metadata": {},
   "source": [
    "# figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d646d3f4-cf38-446e-bd46-5592f1481400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: [['W14', 'ncep', 'observed'], ['W14', 'era5', 'observed'], ['L13', 'ncep', 'observed'], ['L13', 'era5', 'observed']]\n",
      "observedncepW14 observedncepW14_stdev\n",
      "observedera5W14 observedera5W14_stdev\n",
      "observedncepL13 observedncepL13_stdev\n",
      "observedera5L13 observedera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'baseline'], ['W14', 'era5', 'baseline'], ['L13', 'ncep', 'baseline'], ['L13', 'era5', 'baseline']]\n",
      "baselinencepW14 baselinencepW14_stdev\n",
      "baselineera5W14 baselineera5W14_stdev\n",
      "baselinencepL13 baselinencepL13_stdev\n",
      "baselineera5L13 baselineera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'noice'], ['W14', 'era5', 'noice'], ['L13', 'ncep', 'noice'], ['L13', 'era5', 'noice']]\n",
      "noicencepW14 noicencepW14_stdev\n",
      "noiceera5W14 noiceera5W14_stdev\n",
      "noicencepL13 noicencepL13_stdev\n",
      "noiceera5L13 noiceera5L13_stdev\n",
      "inputs: [['W14', 'ncep', '1atm'], ['W14', 'era5', '1atm'], ['L13', 'ncep', '1atm'], ['L13', 'era5', '1atm']]\n",
      "1atmncepW14 1atmncepW14_stdev\n",
      "1atmera5W14 1atmera5W14_stdev\n",
      "1atmncepL13 1atmncepL13_stdev\n",
      "1atmera5L13 1atmera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'medmsl'], ['W14', 'era5', 'medmsl'], ['L13', 'ncep', 'medmsl'], ['L13', 'era5', 'medmsl']]\n",
      "medmslncepW14 medmslncepW14_stdev\n",
      "medmslera5W14 medmslera5W14_stdev\n",
      "medmslncepL13 medmslncepL13_stdev\n",
      "medmslera5L13 medmslera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'meanmsl'], ['W14', 'era5', 'meanmsl'], ['L13', 'ncep', 'meanmsl'], ['L13', 'era5', 'meanmsl']]\n",
      "meanmslncepW14 meanmslncepW14_stdev\n",
      "meanmslera5W14 meanmslera5W14_stdev\n",
      "meanmslncepL13 meanmslncepL13_stdev\n",
      "meanmslera5L13 meanmslera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'medK'], ['W14', 'era5', 'medK'], ['L13', 'ncep', 'medK'], ['L13', 'era5', 'medK']]\n",
      "medKncepW14 medKncepW14_stdev\n",
      "medKera5W14 medKera5W14_stdev\n",
      "medKncepL13 medKncepL13_stdev\n",
      "medKera5L13 medKera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'meanK'], ['W14', 'era5', 'meanK'], ['L13', 'ncep', 'meanK'], ['L13', 'era5', 'meanK']]\n",
      "meanKncepW14 meanKncepW14_stdev\n",
      "meanKera5W14 meanKera5W14_stdev\n",
      "meanKncepL13 meanKncepL13_stdev\n",
      "meanKera5L13 meanKera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'medN2O'], ['W14', 'era5', 'medN2O'], ['L13', 'ncep', 'medN2O'], ['L13', 'era5', 'medN2O']]\n",
      "medN2OncepW14 medN2OncepW14_stdev\n",
      "medN2Oera5W14 medN2Oera5W14_stdev\n",
      "medN2OncepL13 medN2OncepL13_stdev\n",
      "medN2Oera5L13 medN2Oera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'meanN2O'], ['W14', 'era5', 'meanN2O'], ['L13', 'ncep', 'meanN2O'], ['L13', 'era5', 'meanN2O']]\n",
      "meanN2OncepW14 meanN2OncepW14_stdev\n",
      "meanN2Oera5W14 meanN2Oera5W14_stdev\n",
      "meanN2OncepL13 meanN2OncepL13_stdev\n",
      "meanN2Oera5L13 meanN2Oera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'WINDS'], ['W14', 'era5', 'WINDS'], ['L13', 'ncep', 'WINDS'], ['L13', 'era5', 'WINDS']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')\n",
      "/tmp/ipykernel_157309/2299199842.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}stdev\"] = np.sqrt(pN2O_uncertainty_condition**2 + wind_uncertainty_condition**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINDSncepW14 WINDSncepW14_stdev\n",
      "WINDSera5W14 WINDSera5W14_stdev\n",
      "WINDSncepL13 WINDSncepL13_stdev\n",
      "WINDSera5L13 WINDSera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'COMBINED'], ['W14', 'era5', 'COMBINED'], ['L13', 'ncep', 'COMBINED'], ['L13', 'era5', 'COMBINED']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')\n",
      "/tmp/ipykernel_157309/2299199842.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}stdev\"] = np.sqrt(pN2O_uncertainty_condition**2 + wind_uncertainty_condition**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINEDncepW14 COMBINEDncepW14_stdev\n",
      "COMBINEDera5W14 COMBINEDera5W14_stdev\n",
      "COMBINEDncepL13 COMBINEDncepL13_stdev\n",
      "COMBINEDera5L13 COMBINEDera5L13_stdev\n",
      "inputs: [['W14', 'ncep', 'CYCLONES'], ['W14', 'era5', 'CYCLONES'], ['L13', 'ncep', 'CYCLONES'], ['L13', 'era5', 'CYCLONES']]\n",
      "CYCLONESncepW14 CYCLONESncepW14_stdev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')\n",
      "/tmp/ipykernel_157309/2299199842.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}stdev\"] = np.sqrt(pN2O_uncertainty_condition**2 + wind_uncertainty_condition**2)\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYCLONESera5W14 CYCLONESera5W14_stdev\n",
      "CYCLONESncepL13 CYCLONESncepL13_stdev\n",
      "CYCLONESera5L13 CYCLONESera5L13_stdev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[meanlabel] = Ft\n",
      "/tmp/ipykernel_157309/2299199842.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[stdevlabel] = Ftstdev\n",
      "/tmp/ipykernel_157309/2299199842.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')\n",
      "/tmp/ipykernel_157309/2299199842.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"Ft{condition}stdev\"] = np.sqrt(pN2O_uncertainty_condition**2 + wind_uncertainty_condition**2)\n"
     ]
    }
   ],
   "source": [
    "outputpath = 'datasets'\n",
    "t = pq.read_table(f\"{outputpath}/n2opredictions.parquet\")\n",
    "data = t.to_pandas()\n",
    "\n",
    "data[\"msl\"] = data[[\"msl_era5\", \"msl_ncep\"]].mean(axis = 1)\n",
    "data[\"U10\"] = data[[\"U10_era5\",\"U10_ncep\"]].mean(axis = 1)\n",
    "data[\"SI\"] = data.SI_era5\n",
    "\n",
    "# constants for converting umol/m2/day to Tg N2O-N/year\n",
    "Ngpermol = 14.0067\n",
    "Tgperg = 1e12\n",
    "umolpermol = 1e6\n",
    "\n",
    "# properties needed for area-time integration\n",
    "areas = pd.DataFrame([[\"STZ\",2.26e7],[\"SAZ\",1.94e7],[\"PFZ\",1.43e7],\n",
    "                           [\"ASZ\",1.28e7],[\"SIZ\",1.72e7],[\"TOTAL\",8.64e7]],\n",
    "                columns = [\"zone\",\"Area_km2\"]).set_index(\"zone\")\n",
    "\n",
    "areas[\"m2\"] = areas.Area_km2*1e6 # convert to m2 because fluxes are in umol/m2/day\n",
    "\n",
    "daysinmonth = pd.DataFrame([[1.0, 31],\n",
    "                            [2.0,28],\n",
    "                            [3.0,31],\n",
    "                            [4.0,30],\n",
    "                            [5.0,31],\n",
    "                            [6.0,30],\n",
    "                            [7.0, 31],\n",
    "                            [8.0,31],\n",
    "                            [9.0,30],\n",
    "                            [10.0,31],\n",
    "                            [11.0,30],\n",
    "                            [12.0,31],\n",
    "                           ],\n",
    "                           columns = [\"month\",\"daysinmonth\"]).set_index(\"month\")\n",
    "Ft = np.load(f\"{outputpath}/fluxtests/observedcombined_mean.npy\")\n",
    "Ftstdev = np.load(f\"{outputpath}/fluxtests/observedcombined_stdev.npy\")\n",
    "\n",
    "data[\"Ft\"] = Ft\n",
    "data[\"Ftstdev\"] = Ftstdev\n",
    "\n",
    "condition = \"observed\"\n",
    "inputs = generateinputs([condition], [\"ncep\", \"era5\"])\n",
    "labels = []\n",
    "\n",
    "for test in inputs:\n",
    "    p = test[0]\n",
    "    t = test[2]\n",
    "    w = test[1]\n",
    "    \n",
    "    Ft = np.load(f\"{outputpath}/fluxtests/{t}{w}{p}_mean.npy\")\n",
    "    Ftstdev = np.load(f\"{outputpath}/fluxtests/{test[2]}{test[1]}{test[0]}_stdev.npy\")\n",
    "    \n",
    "    meanlabel = f\"{t}{w}{p}\"\n",
    "    stdevlabel = f\"{t}{w}{p}_stdev\"\n",
    "\n",
    "    print(meanlabel, stdevlabel)\n",
    "    labels.append(meanlabel)\n",
    "\n",
    "    data[meanlabel] = Ft\n",
    "    data[stdevlabel] = Ftstdev\n",
    "\n",
    "data[\"Ft\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')\n",
    "data[\"pN2O_uncertainty\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_stdev.npy')\n",
    "data[\"wind_uncertainty\"] = data[labels].std(axis=1)\n",
    "data[\"Ftstdev\"] = np.sqrt(data[\"pN2O_uncertainty\"]**2 + data[\"wind_uncertainty\"]**2)\n",
    "\n",
    "tests = [\"baseline\", \"noice\", \"1atm\", \"medmsl\", \"meanmsl\", \"medK\", \"meanK\", \"medN2O\",\n",
    "    \"meanN2O\", # for whatever reason, this one throws an error\n",
    "    \"WINDS\", \"COMBINED\", \"CYCLONES\"\n",
    "    ]\n",
    "for condition in tests:\n",
    "    inputs = generateinputs([condition], [\"ncep\", \"era5\"])\n",
    "    labels = []\n",
    "    \n",
    "    for test in inputs:\n",
    "        p = test[0]\n",
    "        t = test[2]\n",
    "        w = test[1]\n",
    "        \n",
    "        Ft = np.load(f\"{outputpath}/fluxtests/{t}{w}{p}_mean.npy\")\n",
    "        Ftstdev = np.load(f\"{outputpath}/fluxtests/{test[2]}{test[1]}{test[0]}_stdev.npy\")\n",
    "        \n",
    "        meanlabel = f\"{t}{w}{p}\"\n",
    "        stdevlabel = f\"{t}{w}{p}_stdev\"\n",
    "    \n",
    "        print(meanlabel, stdevlabel)\n",
    "        labels.append(meanlabel)\n",
    "    \n",
    "        data[meanlabel] = Ft\n",
    "        data[stdevlabel] = Ftstdev\n",
    "    \n",
    "    data[f\"Ft{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')\n",
    "    pN2O_uncertainty_condition = np.load(f'{outputpath}/fluxtests/{condition}combined_stdev.npy')\n",
    "    wind_uncertainty_condition = data[labels].std(axis=1)\n",
    "    data[f\"Ft{condition}stdev\"] = np.sqrt(pN2O_uncertainty_condition**2 + wind_uncertainty_condition**2)\n",
    "\n",
    "data = data.copy() # avoid fragmenting dataframe\n",
    "\n",
    "ph2ov = vpress_sw(data.SP,data.pt) # atm\n",
    "f  = fugacity_factor(data.pt, gas='N2O',slp=data.msl)\n",
    "data[\"pN2Oatm\"] = data.XN2Oa*1e9 * f * (data.msl - ph2ov)\n",
    "data[\"pN2Oatm_1atm\"] = data.XN2Oa*1e9 * f * (1 - ph2ov)\n",
    "data[\"DpN2O_pred\"] = data.pN2O_pred -data[\"pN2Oatm\"]\n",
    "data[\"DpN2O_pred2\"] = data.pN2O_pred -data[\"pN2Oatm_1atm\"]\n",
    "data[\"DpN2O_pred3\"] = np.median(data.pN2O_pred) - data[\"pN2Oatm\"]\n",
    "\n",
    "data.to_csv('datasets/fig1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c1fb55-2e3c-4790-8122-627201dfde06",
   "metadata": {},
   "source": [
    "# figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4706790f-2390-4984-ae46-77f85981bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: [['W14', 'ncep', 'observed'], ['W14', 'era5', 'observed'], ['L13', 'ncep', 'observed'], ['L13', 'era5', 'observed']]\n",
      "1.5954010434742338\n"
     ]
    }
   ],
   "source": [
    "outputpath = 'datasets'\n",
    "t = pq.read_table(f\"{outputpath}/n2opredictions.parquet\")\n",
    "surface = t.to_pandas()\n",
    "\n",
    "inputs = generateinputs([\"observed\"], [\"ncep\", \"era5\"])\n",
    "\n",
    "for test in inputs:\n",
    "    p = test[0]\n",
    "    t = test[2]\n",
    "    w = test[1]\n",
    "    \n",
    "    Ft = np.load(f\"{outputpath}/fluxtests/{t}{w}{p}_mean.npy\")\n",
    "    Ftstdev = np.load(f\"{outputpath}/fluxtests/{test[2]}{test[1]}{test[0]}_stdev.npy\")\n",
    "    \n",
    "    meanlabel = f\"{t}{w}{p}\"\n",
    "    stdevlabel = f\"{t}{w}{p}_stdev\"\n",
    "\n",
    "    surface[meanlabel] = Ft*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    surface[stdevlabel] = Ftstdev*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "\n",
    "surface[\"combined_mean\"] = np.load(f'{outputpath}/fluxtests/{\"observed\"}combined_mean.npy')*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "surface[\"combined_uncertainty\"] = np.load(f'{outputpath}/fluxtests/{\"observed\"}combined_stdev.npy')*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "\n",
    "data_sorted = surface.sort_values(f\"msl_era5\")\n",
    "\n",
    "mean_cumsum = np.cumsum(np.array(data_sorted[\"combined_mean\"]))\n",
    "mslarray = np.array(data_sorted[f\"msl_era5\"])\n",
    "\n",
    "print(mean_cumsum[-1])\n",
    "cumsums = np.empty((len(data_sorted),len(inputs)))\n",
    "\n",
    "for count, test in enumerate(inputs):\n",
    "    p = test[0]\n",
    "    t = test[2]\n",
    "    w = test[1]\n",
    "\n",
    "    cumsum = np.cumsum(np.array(data_sorted[f\"{t}{w}{p}\"]))\n",
    "    cumsums[:,count] = cumsum\n",
    "\n",
    "    uncertainties = np.array(data_sorted[f\"{t}{w}{p}_stdev\"])\n",
    "    uncertaintiessquared = uncertainties**2\n",
    "    uncertaintiescumulative = np.cumsum(uncertaintiessquared)\n",
    "    uncertainties = np.sqrt(uncertaintiescumulative)\n",
    "\n",
    "wind_uncertainty = np.std(cumsums, axis = 1)\n",
    "\n",
    "montecarlo_uncertainty = np.cumsum(np.array(data_sorted[\"combined_uncertainty\"])**2)\n",
    "\n",
    "bootstrap_uncertainty = 0.038*mean_cumsum#bootstrap_uncertainties[260]/np.mean(bootstrap_sums[260,:])*mean_cumsum\n",
    "\n",
    "total_uncertainty = np.sqrt(\n",
    "    wind_uncertainty**2 + \n",
    "    montecarlo_uncertainty + \n",
    "    bootstrap_uncertainty**2\n",
    ")\n",
    "\n",
    "data_sorted[\"mslarray\"] = mslarray\n",
    "data_sorted[\"mean_cumsum\"] = mean_cumsum\n",
    "data_sorted[\"wind_uncertainty\"] = wind_uncertainty\n",
    "data_sorted[\"montecarlo_uncertainty\"] = montecarlo_uncertainty\n",
    "data_sorted[\"bootstrap_uncertainty\"] = bootstrap_uncertainty\n",
    "data_sorted[\"total_uncertainty\"] = total_uncertainty\n",
    "data_sorted.to_csv(\"datasets/fig2a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb046342-0e9a-49a3-b310-8cdc7f357354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: [['W14', 'ncep', '1atm'], ['W14', 'era5', '1atm'], ['L13', 'ncep', '1atm'], ['L13', 'era5', '1atm']]\n",
      "0.8929258341553027\n"
     ]
    }
   ],
   "source": [
    "outputpath = 'datasets'\n",
    "t = pq.read_table(f\"{outputpath}/n2opredictions.parquet\")\n",
    "surface = t.to_pandas()\n",
    "\n",
    "inputs = generateinputs([\"1atm\"], [\"ncep\", \"era5\"])\n",
    "\n",
    "for test in inputs:\n",
    "    p = test[0]\n",
    "    t = test[2]\n",
    "    w = test[1]\n",
    "    \n",
    "    Ft = np.load(f\"{outputpath}/fluxtests/{t}{w}{p}_mean.npy\")\n",
    "    Ftstdev = np.load(f\"{outputpath}/fluxtests/{test[2]}{test[1]}{test[0]}_stdev.npy\")\n",
    "    \n",
    "    meanlabel = f\"{t}{w}{p}\"\n",
    "    stdevlabel = f\"{t}{w}{p}_stdev\"\n",
    "\n",
    "    surface[meanlabel] = Ft*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    surface[stdevlabel] = Ftstdev*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "\n",
    "surface[\"combined_mean\"] = np.load(f'{outputpath}/fluxtests/{\"1atm\"}combined_mean.npy')*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "surface[\"combined_uncertainty\"] = np.load(f'{outputpath}/fluxtests/{\"1atm\"}combined_stdev.npy')*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "\n",
    "data_sorted = surface.sort_values(f\"msl_era5\")\n",
    "\n",
    "mean_cumsum = np.cumsum(np.array(data_sorted[\"combined_mean\"]))\n",
    "mslarray = np.array(data_sorted[f\"msl_era5\"])\n",
    "\n",
    "print(mean_cumsum[-1])\n",
    "cumsums = np.empty((len(data_sorted),len(inputs)))\n",
    "\n",
    "for count, test in enumerate(inputs):\n",
    "    p = test[0]\n",
    "    t = test[2]\n",
    "    w = test[1]\n",
    "\n",
    "    cumsum = np.cumsum(np.array(data_sorted[f\"{t}{w}{p}\"]))\n",
    "    cumsums[:,count] = cumsum\n",
    "\n",
    "    uncertainties = np.array(data_sorted[f\"{t}{w}{p}_stdev\"])\n",
    "    uncertaintiessquared = uncertainties**2\n",
    "    uncertaintiescumulative = np.cumsum(uncertaintiessquared)\n",
    "    uncertainties = np.sqrt(uncertaintiescumulative)\n",
    "\n",
    "wind_uncertainty = np.std(cumsums, axis = 1)\n",
    "\n",
    "montecarlo_uncertainty = np.cumsum(np.array(data_sorted[\"combined_uncertainty\"])**2)\n",
    "\n",
    "bootstrap_uncertainty = 0.038*mean_cumsum#bootstrap_uncertainties[260]/np.mean(bootstrap_sums[260,:])*mean_cumsum\n",
    "\n",
    "total_uncertainty = np.sqrt(\n",
    "    wind_uncertainty**2 + \n",
    "    montecarlo_uncertainty + \n",
    "    bootstrap_uncertainty**2\n",
    ")\n",
    "\n",
    "data_sorted[\"mslarray\"] = mslarray\n",
    "data_sorted[\"mean_cumsum\"] = mean_cumsum\n",
    "data_sorted[\"wind_uncertainty\"] = wind_uncertainty\n",
    "data_sorted[\"montecarlo_uncertainty\"] = montecarlo_uncertainty\n",
    "data_sorted[\"bootstrap_uncertainty\"] = bootstrap_uncertainty\n",
    "data_sorted[\"total_uncertainty\"] = total_uncertainty\n",
    "data_sorted.to_csv(\"datasets/fig2b.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91323cd1-2b4a-4519-8252-f545deed4da8",
   "metadata": {},
   "source": [
    "# figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff611fb2-6c53-4bc2-9f5f-dd4984a142db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: [['W14', 'ncep', 'observed'], ['W14', 'era5', 'observed'], ['L13', 'ncep', 'observed'], ['L13', 'era5', 'observed']]\n",
      "0.3211880859077775\n",
      "inputs: [['W14', 'ncep', 'medK'], ['W14', 'era5', 'medK'], ['L13', 'ncep', 'medK'], ['L13', 'era5', 'medK']]\n",
      "0.13933789074149894\n",
      "inputs: [['W14', 'ncep', '1atm'], ['W14', 'era5', '1atm'], ['L13', 'ncep', '1atm'], ['L13', 'era5', '1atm']]\n",
      "0.33390530737762725\n",
      "inputs: [['W14', 'ncep', 'baseline'], ['W14', 'era5', 'baseline'], ['L13', 'ncep', 'baseline'], ['L13', 'era5', 'baseline']]\n",
      "0.10866104187122427\n"
     ]
    }
   ],
   "source": [
    "conditions = [\"observed\", \"medK\", \"1atm\", \"baseline\"]\n",
    "totals = []\n",
    "mids = []\n",
    "data_sorted = surface.sort_values(f\"msl_era5\")\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    inputs = generateinputs([condition], [\"ncep\", \"era5\"])\n",
    "    \n",
    "    for test in inputs:\n",
    "        p = test[0]\n",
    "        t = test[2]\n",
    "        w = test[1]\n",
    "        \n",
    "        Ft = np.load(f\"{outputpath}/fluxtests/{t}{w}{p}_mean.npy\")\n",
    "        Ftstdev = np.load(f\"{outputpath}/fluxtests/{test[2]}{test[1]}{test[0]}_stdev.npy\")\n",
    "        \n",
    "        meanlabel = f\"{t}{w}{p}\"\n",
    "        stdevlabel = f\"{t}{w}{p}_stdev\"\n",
    "    \n",
    "        data_sorted[meanlabel] = Ft*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "        data_sorted[stdevlabel] = Ftstdev*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    \n",
    "    data_sorted[\"combined_mean\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    data_sorted[\"combined_uncertainty\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_stdev.npy')*surface.m2*surface.daysinmonth/surface.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    \n",
    "    mean_cumsum = np.cumsum(np.array(data_sorted[\"combined_mean\"]))\n",
    "    data_sorted[f\"Ft_cumulative{condition}\"] = mean_cumsum\n",
    "    \n",
    "    mslarray = np.array(data_sorted[f\"msl_era5\"])\n",
    "\n",
    "    totals.append(mean_cumsum[-1])\n",
    "    msl50 = np.median(mslarray)\n",
    "    mslmask = data_sorted[f\"msl_era5\"]<=msl50\n",
    "    mids.append(mean_cumsum[mslmask][-1])\n",
    "    \n",
    "    cumsums = np.empty((len(data_sorted),len(inputs)))\n",
    "    \n",
    "    for count, test in enumerate(inputs):\n",
    "        p = test[0]\n",
    "        t = test[2]\n",
    "        w = test[1]\n",
    "    \n",
    "        cumsum = np.cumsum(np.array(data_sorted[f\"{t}{w}{p}\"]))\n",
    "        cumsums[:,count] = cumsum\n",
    "    \n",
    "    wind_uncertainty = np.std(cumsums, axis = 1)\n",
    "    \n",
    "    montecarlo_uncertainty = np.cumsum(np.array(data_sorted[\"combined_uncertainty\"])**2)\n",
    "    \n",
    "    bootstrap_uncertainty = 0.0385*mean_cumsum#bootstrap_uncertainties[260]/np.mean(bootstrap_sums[260,:])*mean_cumsum\n",
    "    \n",
    "    total_uncertainty = np.sqrt(\n",
    "        wind_uncertainty**2 + \n",
    "        montecarlo_uncertainty + \n",
    "        bootstrap_uncertainty**2\n",
    "    )\n",
    "    print(total_uncertainty[-1])\n",
    "\n",
    "data_sorted.to_csv(\"datasets/fig3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b2f12-1cb6-4791-abd1-3745c947aafa",
   "metadata": {},
   "source": [
    "# figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b625241b-368e-4317-ac65-dda446a226ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: [['W14', 'ncep', '1atm'], ['W14', 'era5', '1atm'], ['L13', 'ncep', '1atm'], ['L13', 'era5', '1atm']]\n",
      "inputs: [['W14', 'ncep', 'medK'], ['W14', 'era5', 'medK'], ['L13', 'ncep', 'medK'], ['L13', 'era5', 'medK']]\n",
      "inputs: [['W14', 'ncep', 'medN2O'], ['W14', 'era5', 'medN2O'], ['L13', 'ncep', 'medN2O'], ['L13', 'era5', 'medN2O']]\n",
      "inputs: [['W14', 'ncep', 'observed'], ['W14', 'era5', 'observed'], ['L13', 'ncep', 'observed'], ['L13', 'era5', 'observed']]\n",
      "inputs: [['W14', 'ncep', 'noice'], ['W14', 'era5', 'noice'], ['L13', 'ncep', 'noice'], ['L13', 'era5', 'noice']]\n",
      "inputs: [['W14', 'ncep', 'WINDS'], ['W14', 'era5', 'WINDS'], ['L13', 'ncep', 'WINDS'], ['L13', 'era5', 'WINDS']]\n",
      "inputs: [['W14', 'ncep', 'COMBINED'], ['W14', 'era5', 'COMBINED'], ['L13', 'ncep', 'COMBINED'], ['L13', 'era5', 'COMBINED']]\n"
     ]
    }
   ],
   "source": [
    "conditions = [\"1atm\", \"medK\", \"medN2O\", \"observed\", \"noice\", \"WINDS\", \"COMBINED\"]\n",
    "monthlycolumns = ['zone', 'month']\n",
    "seasonalcolumns = ['zone', 'season']\n",
    "annualcolumns = ['month']\n",
    "zonalcolumns = ['zone']\n",
    "\n",
    "wind_uncertainties_monthly = []\n",
    "wind_uncertainties_annual = []\n",
    "wind_uncertainties_seasonal = []\n",
    "wind_uncertainties_zonal = []\n",
    "\n",
    "annual = data.copy()\n",
    "seasonarray = np.copy(np.array(data.zone))\n",
    "seasonarray[np.isin(annual.month, [4,5,6])] = 'peak'\n",
    "seasonarray[np.isin(annual.month, [10,11,12])] = 'trough'\n",
    "seasonarray[np.isin(annual.month, [1,2,3,7,8,9])] = 'none'\n",
    "annual['season'] = seasonarray\n",
    "annual[\"doy\"] = np.array([pd.Timestamp(d).dayofyear for d in annual.loc[:,'JULD'].values])\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    inputs = generateinputs([condition], [\"ncep\", \"era5\"])\n",
    "    monthlylabels = ['zone', 'month']\n",
    "    seasonallabels = ['zone', 'season']\n",
    "    annuallabels = ['month']\n",
    "    zonallabels = ['zone']\n",
    "    \n",
    "    for test in inputs:\n",
    "        p = test[0]\n",
    "        w = test[1]\n",
    "        t = test[2]\n",
    "        \n",
    "        Ft = np.load(f\"{outputpath}/fluxtests/{t}{w}{p}_mean.npy\")\n",
    "        \n",
    "        meanlabel = f\"{t}{w}{p}\"\n",
    "        for lst in [monthlylabels, seasonallabels, annuallabels, zonallabels]:\n",
    "            lst.append(meanlabel)\n",
    "\n",
    "        annual[meanlabel] = Ft*annual.m2*annual.daysinmonth/annual.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "\n",
    "    wind_uncertainty = np.array(annual[monthlylabels].groupby(['zone','month']).sum().std(axis = 1))\n",
    "    wind_uncertainties_monthly.append(wind_uncertainty)\n",
    "\n",
    "    wind_uncertainty = np.array(annual[annuallabels].groupby('month').sum().std(axis = 1))\n",
    "    wind_uncertainties_annual.append(wind_uncertainty)\n",
    "\n",
    "    wind_uncertainty = np.array(annual[seasonallabels].groupby(['zone','season']).sum().std(axis = 1))*4\n",
    "    wind_uncertainties_seasonal.append(wind_uncertainty)\n",
    "\n",
    "    wind_uncertainty = np.array(annual[zonallabels].groupby('zone').sum().std(axis = 1))\n",
    "    wind_uncertainties_zonal.append(wind_uncertainty)\n",
    "    \n",
    "    annual[f\"mean_flux{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_mean.npy')*annual.m2*annual.daysinmonth/annual.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    annual[f\"montecarlo_uncertainty{condition}\"] = np.load(f'{outputpath}/fluxtests/{condition}combined_stdev.npy')*annual.m2*annual.daysinmonth/annual.msl_era5count*(Ngpermol*2)/Tgperg/umolpermol\n",
    "    annual[f\"montecarlo_uncertainty_squared{condition}\"] = annual[f\"montecarlo_uncertainty{condition}\"]**2\n",
    "\n",
    "    for lst in [monthlycolumns, seasonalcolumns, annualcolumns, zonalcolumns]:\n",
    "        lst.append(f\"mean_flux{condition}\")\n",
    "        lst.append(f\"montecarlo_uncertainty_squared{condition}\")\n",
    "\n",
    "grouped = annual[monthlycolumns].groupby(['zone','month']).sum()\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    grouped[f\"wind_uncertainty{condition}\"] = wind_uncertainties_monthly[i]\n",
    "    grouped[f\"bootstrap_uncertainty{condition}\"] = 0.0385*grouped[f\"mean_flux{condition}\"]\n",
    "    grouped[f\"total_uncertainty{condition}\"] = np.sqrt(grouped[f\"montecarlo_uncertainty_squared{condition}\"] + \n",
    "                                                       grouped[f\"wind_uncertainty{condition}\"]**2 +\n",
    "                                                       grouped[f\"bootstrap_uncertainty{condition}\"]**2)\n",
    "\n",
    "overall = annual[annualcolumns].groupby(['month']).sum()\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    overall[f\"wind_uncertainty{condition}\"] = wind_uncertainties_annual[i]\n",
    "    overall[f\"bootstrap_uncertainty{condition}\"] = 0.0385*overall[f\"mean_flux{condition}\"]\n",
    "    overall[f\"total_uncertainty{condition}\"] = np.sqrt(overall[f\"montecarlo_uncertainty_squared{condition}\"] + \n",
    "                                                       overall[f\"wind_uncertainty{condition}\"]**2 +\n",
    "                                                       overall[f\"bootstrap_uncertainty{condition}\"]**2)\n",
    "\n",
    "overall.to_csv(\"datasets/fig4.csv\")\n",
    "grouped.to_csv('datasets/fig5column2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde97fdf-94bf-4f21-ac8c-b31d90550d3e",
   "metadata": {},
   "source": [
    "# figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cdb9e17-21a4-40f6-b0bc-57b40cb3ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyzonalmeans = data[[\"zone\", \"month\", \"DpN2O_pred\", \"DpN2O_pred2\", \"DpN2O_pred3\"]].groupby([\"zone\", \"month\"]).mean()\n",
    "monthlyzonalstds = data[[\"zone\", \"month\", \"DpN2O_pred\", \"DpN2O_pred2\", \"DpN2O_pred3\"]].groupby([\"zone\", \"month\"]).std()\n",
    "monthlyzonalcounts = data[[\"zone\", \"month\", \"DpN2O_pred\", \"DpN2O_pred2\", \"DpN2O_pred3\"]].groupby([\"zone\", \"month\"]).count()\n",
    "monthlyzonalmeans = monthlyzonalmeans.join(monthlyzonalstds, rsuffix = \"_std\")\n",
    "monthlyzonalmeans = monthlyzonalmeans.join(monthlyzonalcounts, rsuffix = \"_count\")\n",
    "monthlyzonalmeans['DpN2O_pred_sem'] = monthlyzonalmeans['DpN2O_pred_std']/np.sqrt(monthlyzonalmeans.DpN2O_pred_count)\n",
    "monthlyzonalmeans['DpN2O_pred2_sem'] = monthlyzonalmeans['DpN2O_pred2_std']/np.sqrt(monthlyzonalmeans.DpN2O_pred2_count)\n",
    "monthlyzonalmeans['DpN2O_pred3_sem'] = monthlyzonalmeans['DpN2O_pred3_std']/np.sqrt(monthlyzonalmeans.DpN2O_pred3_count)\n",
    "\n",
    "monthlyzonalmeans.to_csv('datasets/fig5.csv')\n",
    "\n",
    "grouped = annual[monthlycolumns].groupby(['zone','month']).sum()\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    grouped[f\"wind_uncertainty{condition}\"] = wind_uncertainties_monthly[i]\n",
    "    grouped[f\"bootstrap_uncertainty{condition}\"] = 0.0385*grouped[f\"mean_flux{condition}\"]\n",
    "    grouped[f\"total_uncertainty{condition}\"] = np.sqrt(grouped[f\"montecarlo_uncertainty_squared{condition}\"] + \n",
    "                                                       grouped[f\"wind_uncertainty{condition}\"]**2 +\n",
    "                                                       grouped[f\"bootstrap_uncertainty{condition}\"]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5d0c4-ff9a-4683-aa97-9cae12fb8f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
